# Local AI Demo - æ¼”ç¤ºè¯´æ˜ / Demo Instructions

## ä¸­æ–‡è¯´æ˜ / Chinese Instructions

### é¡¹ç›®ç®€ä»‹
è¿™æ˜¯ä¸€ä¸ªæœ¬åœ°AIæ¼”ç¤ºé¡¹ç›®ï¼Œæ”¯æŒä¸æœ¬åœ°AIæ¨¡å‹è¿›è¡Œäº¤äº’çš„Webç•Œé¢ã€‚å…·æœ‰ä¸“ä¸šçš„æ·±è‰²ä¸»é¢˜è®¾è®¡ï¼Œæ”¯æŒå¤šç§LLMæä¾›å•†ã€‚

### å¿«é€Ÿå¼€å§‹
1. **å¯åŠ¨OllamaæœåŠ¡**ï¼š
   ```bash
   ./start-ollama-cors.sh
   ```

2. **æ‰“å¼€æ¼”ç¤ºé¡µé¢**ï¼š
   - ç›´æ¥æ‰“å¼€ `index.html` æ–‡ä»¶
   - æˆ–è¿è¡Œæœ¬åœ°æœåŠ¡å™¨ï¼š`python3 -m http.server 3000`

3. **é…ç½®æ¨¡å‹**ï¼š
   - ç‚¹å‡»é¡µé¢é¡¶éƒ¨çš„ "Connect Model" æŒ‰é’®
   - é€‰æ‹©æä¾›å•†ï¼šOllama
   - åŸºç¡€URLï¼š`http://127.0.0.1:11434`
   - æ¨¡å‹ï¼š`gemma3:1b`
   - ç‚¹å‡» "Test" æµ‹è¯•è¿æ¥
   - ç‚¹å‡» "Save" ä¿å­˜é…ç½®

4. **å¼€å§‹èŠå¤©**ï¼š
   - åœ¨èŠå¤©æ¡†ä¸­è¾“å…¥æ¶ˆæ¯
   - æŒ‰å›è½¦é”®æˆ–ç‚¹å‡»å‘é€æŒ‰é’®

### æ•…éšœæ’é™¤
- **CORSé”™è¯¯**ï¼šè¿è¡Œ `python3 cors_proxy.py` å¯åŠ¨ä»£ç†æœåŠ¡å™¨
- **è¿æ¥å¤±è´¥**ï¼šç¡®ä¿OllamaæœåŠ¡æ­£åœ¨è¿è¡Œå¹¶ä¸”ç«¯å£æ­£ç¡®
- **æ— å“åº”**ï¼šæ£€æŸ¥æ¨¡å‹åç§°æ˜¯å¦æ­£ç¡®

---

## English Instructions

### Project Overview
This is a local AI demo project featuring a web interface for interacting with local AI models. It includes a professional dark theme design and supports multiple LLM providers.

### Quick Start
1. **Start Ollama Service**:
   ```bash
   ./start-ollama-cors.sh
   ```

2. **Open Demo Page**:
   - Open `index.html` file directly
   - Or run local server: `python3 -m http.server 3000`

3. **Configure Model**:
   - Click "Connect Model" button at the top of the page
   - Select Provider: Ollama
   - Base URL: `http://127.0.0.1:11434`
   - Model: `gemma3:1b`
   - Click "Test" to test connection
   - Click "Save" to save configuration

4. **Start Chatting**:
   - Type message in chat box
   - Press Enter or click Send button

### Troubleshooting
- **CORS Error**: Run `python3 cors_proxy.py` to start proxy server
- **Connection Failed**: Ensure Ollama service is running and port is correct
- **No Response**: Check if model name is correct

### Features
- ğŸ¨ Professional dark theme UI
- ğŸ”„ Real-time chat interface
- ğŸŒ Multi-provider support (Ollama, OpenAI, llama.cpp)
- ğŸ“± Responsive design
- ğŸ”§ Easy configuration
- ğŸš€ Quick prompts for common scenarios

### Presentation Tips
1. **Demo Flow**:
   - Show the sophisticated UI design
   - Demonstrate model configuration process
   - Test real-time chat functionality
   - Highlight offline capabilities
   - Show multi-language support

2. **Key Points to Emphasize**:
   - Local deployment (no internet required)
   - Privacy-focused (data stays local)
   - Professional interface design
   - Easy setup and configuration
   - Cross-platform compatibility

3. **Technical Highlights**:
   - CORS handling for browser compatibility
   - Support for multiple AI providers
   - Responsive web design
   - Real-time typing indicators
   - Smooth scrolling chat interface